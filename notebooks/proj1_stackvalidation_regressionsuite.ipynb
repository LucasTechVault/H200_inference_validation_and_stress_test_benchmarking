{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6683e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, platform, subprocess, textwrap\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, List\n",
    "\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a90428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ebed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS_DIR = Path.cwd() / \"runs\"\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f421f728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/lucasleow/project-titan/proj1_stackvalidation_regressionsuite/runs')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUNS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d881e647",
   "metadata": {},
   "source": [
    "# Section 1 - Profiling Hardware\n",
    "\n",
    "### Goal - Single JSON Snapshot to execute before experiment runs\n",
    "- Environment / Stack\n",
    "- GPU Topo / Interconnect\n",
    "- Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd203d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shell command runner with exception handling for json\n",
    "def run_cmd(cmd: List[str], timeout: int = 30) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - subprocess.run(...) -> raises FileNotFound if command doesn't exist\n",
    "        - wrap with try-except, return -1 if command doesn't exist\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = subprocess.run(\n",
    "            cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=timeout, # 30 seconds timeout in case command hangs\n",
    "            text=True,\n",
    "            check=False\n",
    "        )\n",
    "        return {\n",
    "            \"cmd\": \" \".join(cmd),\n",
    "            \"returncode\": p.returncode,\n",
    "            \"stdout\": p.stdout.strip(), # save STDOUT and STDERR into dict for json dump\n",
    "            \"stderr\": p.stderr.strip(),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"cmd\": \" \".join(cmd),\n",
    "            \"returncode\": -1,\n",
    "            \"stdout\": \"\",\n",
    "            \"stderr\": f\"{type(e).__name__}: {e}\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e094c432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'torch_version': '2.9.0+cu128',\n",
       " 'torch_cuda_build': '12.8',\n",
       " 'cuda_available': True,\n",
       " 'device_count': 2,\n",
       " 'nccl_version': (2, 27, 5),\n",
       " 'gpus': [{'index': 0,\n",
       "   'name': 'NVIDIA H200 NVL',\n",
       "   'total_memory_gb': 139.8,\n",
       "   'sm_count': 132,\n",
       "   'compute_capability': '9.0',\n",
       "   'bf16_supported': True},\n",
       "  {'index': 1,\n",
       "   'name': 'NVIDIA H200 NVL',\n",
       "   'total_memory_gb': 139.8,\n",
       "   'sm_count': 132,\n",
       "   'compute_capability': '9.0',\n",
       "   'bf16_supported': True}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# System Introspection\n",
    "# Software + Hardware stack\n",
    "\n",
    "def torch_stack_info() -> Dict[str, Any]:\n",
    "    info = {\n",
    "        \"torch_version\": torch.__version__, # Version of PyTorch \n",
    "        \"torch_cuda_build\": torch.version.cuda, # Version of CUDA PyTorch compiled with - if mismatch with GPU CUDA Driver, might cause performance regression\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"device_count\": torch.cuda.device_count(),\n",
    "    }\n",
    "\n",
    "    # NCCL version (best-effort)\n",
    "    nccl_ver = None\n",
    "    try:\n",
    "        # PyTorch exposes NCCL version here on many builds\n",
    "        # Older NCCL versions might not use NVLink, defaulting to PCIe speeds\n",
    "        nccl_ver = torch.cuda.nccl.version()\n",
    "    except Exception:\n",
    "        pass\n",
    "    info[\"nccl_version\"] = nccl_ver\n",
    "\n",
    "    # GPU properties\n",
    "    gpus = []\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            prop = torch.cuda.get_device_properties(i)\n",
    "            gpus.append({\n",
    "                \"index\": i,\n",
    "                \"name\": prop.name,\n",
    "                \"total_memory_gb\": round(prop.total_memory / (1024**3), 2),\n",
    "                \"sm_count\": getattr(prop, \"multi_processor_count\", None),\n",
    "                \"compute_capability\": f\"{prop.major}.{prop.minor}\",\n",
    "                \"bf16_supported\": bool(getattr(prop, \"major\", 0) >= 8),  # heuristic\n",
    "            })\n",
    "    info[\"gpus\"] = gpus\n",
    "    return info\n",
    "\n",
    "torch_stack_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847e44d",
   "metadata": {},
   "source": [
    "### Understanding torch_stack_info:\n",
    "```\n",
    "- torch_version --> 2.9.0+cu128\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a392f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P2P - NVLink Check\n",
    "# Turn on NVLink if required\n",
    "def p2p_matrix() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Builds P2P access matrix using torch.\n",
    "    Also attempts to enable peer access (best-effort).\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\"available\": False, \"reason\": \"torch.cuda.is_available() is False\"}\n",
    "\n",
    "    n = torch.cuda.device_count()\n",
    "    can = [[None for _ in range(n)] for _ in range(n)]\n",
    "    enabled_attempts = []\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                can[i][j] = True\n",
    "                continue\n",
    "            try:\n",
    "                can[i][j] = bool(torch.cuda.can_device_access_peer(i, j))\n",
    "            except Exception as e:\n",
    "                can[i][j] = f\"ERR:{type(e).__name__}:{e}\"\n",
    "\n",
    "    # Try enabling peer access (optional; doesnâ€™t require root)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            try:\n",
    "                torch.cuda.set_device(i)\n",
    "                # enable_peer_access can throw if already enabled / unsupported\n",
    "                torch.cuda.enable_peer_access(j)\n",
    "                enabled_attempts.append({\"from\": i, \"to\": j, \"enabled\": True})\n",
    "            except Exception as e:\n",
    "                enabled_attempts.append({\"from\": i, \"to\": j, \"enabled\": False, \"err\": f\"{type(e).__name__}: {e}\"})\n",
    "\n",
    "    # Re-check after enable attempts\n",
    "    can_after = [[None for _ in range(n)] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                can_after[i][j] = True\n",
    "                continue\n",
    "            try:\n",
    "                can_after[i][j] = bool(torch.cuda.can_device_access_peer(i, j))\n",
    "            except Exception as e:\n",
    "                can_after[i][j] = f\"ERR:{type(e).__name__}:{e}\"\n",
    "\n",
    "    return {\n",
    "        \"available\": True,\n",
    "        \"can_access_peer_before\": can,\n",
    "        \"enable_peer_access_attempts\": enabled_attempts,\n",
    "        \"can_access_peer_after\": can_after,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd1d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvidia_smi_bundle() -> Dict[str, Any]:\n",
    "    bundle = {}\n",
    "\n",
    "    bundle[\"nvidia_smi\"] = run_cmd([\"nvidia-smi\"])\n",
    "    bundle[\"topo_matrix\"] = run_cmd([\"nvidia-smi\", \"topo\", \"-m\"])\n",
    "\n",
    "    # Structured GPU query: health + clocks + ecc + power\n",
    "    query_fields = [\n",
    "        \"index\", \"name\", \"uuid\", \"pci.bus_id\",\n",
    "        \"temperature.gpu\",\n",
    "        \"utilization.gpu\", \"utilization.memory\",\n",
    "        \"memory.total\", \"memory.used\",\n",
    "        \"clocks.sm\", \"clocks.mem\",\n",
    "        \"power.draw\", \"power.limit\",\n",
    "        \"ecc.mode.current\",\n",
    "        \"clocks_throttle_reasons.active\",\n",
    "    ]\n",
    "    bundle[\"query_gpu\"] = run_cmd([\n",
    "        \"nvidia-smi\",\n",
    "        f\"--query-gpu={','.join(query_fields)}\",\n",
    "        \"--format=csv,noheader,nounits\"\n",
    "    ])\n",
    "\n",
    "    # NVLink status (command support varies by driver)\n",
    "    bundle[\"nvlink_summary\"] = run_cmd([\"nvidia-smi\", \"nvlink\", \"-s\"])\n",
    "    # Per-GPU NVLink detail (best-effort)\n",
    "    bundle[\"nvlink_gpu0\"] = run_cmd([\"nvidia-smi\", \"nvlink\", \"-i\", \"0\"])\n",
    "    bundle[\"nvlink_gpu1\"] = run_cmd([\"nvidia-smi\", \"nvlink\", \"-i\", \"1\"])\n",
    "\n",
    "    return bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b15b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_section1_checks(snapshot: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    checks = []\n",
    "    torch_info = snapshot.get(\"torch\", {})\n",
    "    p2p = snapshot.get(\"p2p\", {})\n",
    "    topo = snapshot.get(\"nvidia\", {}).get(\"topo_matrix\", {}).get(\"stdout\", \"\")\n",
    "\n",
    "    # Check: 2 GPUs\n",
    "    gpu_count = torch_info.get(\"device_count\", 0)\n",
    "    checks.append({\n",
    "        \"name\": \"cuda_visible_2_gpus\",\n",
    "        \"pass\": (gpu_count == 2),\n",
    "        \"details\": f\"torch sees device_count={gpu_count}\"\n",
    "    })\n",
    "\n",
    "    # Check: CUDA available\n",
    "    checks.append({\n",
    "        \"name\": \"cuda_available\",\n",
    "        \"pass\": bool(torch_info.get(\"cuda_available\", False)),\n",
    "        \"details\": f\"torch.cuda.is_available()={torch_info.get('cuda_available')}\"\n",
    "    })\n",
    "\n",
    "    # Check: P2P both directions\n",
    "    p2p_ok = False\n",
    "    p2p_after = p2p.get(\"can_access_peer_after\")\n",
    "    if isinstance(p2p_after, list) and len(p2p_after) >= 2:\n",
    "        p2p_ok = (p2p_after[0][1] is True) and (p2p_after[1][0] is True)\n",
    "\n",
    "    checks.append({\n",
    "        \"name\": \"p2p_enabled_bidirectional\",\n",
    "        \"pass\": bool(p2p_ok),\n",
    "        \"details\": f\"can_access_peer_after[0][1]={p2p_after[0][1] if p2p_after else None}, \"\n",
    "                   f\"[1][0]={p2p_after[1][0] if p2p_after else None}\"\n",
    "    })\n",
    "\n",
    "    # Check: NVLink present in topo matrix\n",
    "    # topo -m typically shows \"NV#\" tokens when NVLink exists.\n",
    "    nvlink_present = (\"NV\" in topo) and (\"GPU0\" in topo) and (\"GPU1\" in topo)\n",
    "    checks.append({\n",
    "        \"name\": \"topology_indicates_nvlink\",\n",
    "        \"pass\": bool(nvlink_present),\n",
    "        \"details\": \"Found 'NV' token in nvidia-smi topo -m output\" if nvlink_present else \"No NV token detected in topo -m\"\n",
    "    })\n",
    "\n",
    "    overall_pass = all(c[\"pass\"] for c in checks)\n",
    "\n",
    "    return {\"overall_pass\": overall_pass, \"checks\": checks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de30cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/lucasleow/project-titan/proj1_stackvalidation_regressionsuite/runs/run_20260108_124018/baseline_snapshot.json\n",
      "\n",
      "=== Section 1 Gate ===\n",
      "OVERALL: PASS\n",
      "- cuda_visible_2_gpus: PASS | torch sees device_count=2\n",
      "- cuda_available: PASS | torch.cuda.is_available()=True\n",
      "- p2p_enabled_bidirectional: PASS | can_access_peer_after[0][1]=True, [1][0]=True\n",
      "- topology_indicates_nvlink: PASS | Found 'NV' token in nvidia-smi topo -m output\n"
     ]
    }
   ],
   "source": [
    "def collect_section1_snapshot() -> Dict[str, Any]:\n",
    "    snap = {\n",
    "        \"ts_unix\": time.time(),\n",
    "        \"ts_readable\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"host\": {\n",
    "            \"hostname\": platform.node(),\n",
    "            \"platform\": platform.platform(),\n",
    "            \"kernel\": platform.release(),\n",
    "            \"python\": platform.python_version(),\n",
    "            \"cwd\": str(Path.cwd()),\n",
    "            \"env\": {\n",
    "                \"CONDA_DEFAULT_ENV\": os.environ.get(\"CONDA_DEFAULT_ENV\"),\n",
    "                \"VIRTUAL_ENV\": os.environ.get(\"VIRTUAL_ENV\"),\n",
    "                \"CUDA_VISIBLE_DEVICES\": os.environ.get(\"CUDA_VISIBLE_DEVICES\"),\n",
    "            },\n",
    "        },\n",
    "        \"torch\": torch_stack_info(),\n",
    "        \"p2p\": p2p_matrix(),\n",
    "        \"nvidia\": nvidia_smi_bundle(),\n",
    "    }\n",
    "    snap[\"section1_gate\"] = derive_section1_checks(snap)\n",
    "    return snap\n",
    "\n",
    "run_id = time.strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "out_dir = RUNS_DIR / run_id\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot = collect_section1_snapshot()\n",
    "snapshot_path = out_dir / \"baseline_snapshot.json\"\n",
    "snapshot_path.write_text(json.dumps(snapshot, indent=2))\n",
    "\n",
    "print(\"Saved:\", snapshot_path)\n",
    "print(\"\\n=== Section 1 Gate ===\")\n",
    "print(\"OVERALL:\", \"PASS\" if snapshot[\"section1_gate\"][\"overall_pass\"] else \"FAIL\")\n",
    "for c in snapshot[\"section1_gate\"][\"checks\"]:\n",
    "    print(f\"- {c['name']}: {'PASS' if c['pass'] else 'FAIL'} | {c['details']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d20b8",
   "metadata": {},
   "source": [
    "## Analysis of Record\n",
    "\n",
    "1. Environment Mapping\n",
    "    - CUDA_VISIBLE_DEVICE \"2, 3\"\n",
    "    - torch.gpus index 0 and 1 (map torch 0 to gpu 2 and torch 1 to gpu 3)\n",
    "    - nvidia.query_gpu -> 0, 1, 2, 3\n",
    "\n",
    "2. Topo\n",
    "    - NV6 between all GPUs (full mesh) - 6 lanes for communication\n",
    "    - NCCL all-reduce and multi-GPU training depend heavily on this\n",
    "\n",
    "    - nvlink Link 0 through Link 17, all active at 26 GB/s - 477GBps unidirection, 900 GBps bidirection\n",
    "        - per link theoretical speed (links are up, negotiated speed is high)\n",
    "        - look out for missing links / links showing 0 GBps / inconsistent speed across links\n",
    "    \n",
    "    - Instead of PIX (PCIe internal switch) -> Fastest possible PCIe - GPUDirect P2P (allocated memory)\n",
    "    - PHB (PCIe Host Bridge) -> travel to CPU bus then to PCIe to another GPU\n",
    "\n",
    "3. Hardware Health\n",
    "    - 143771 MiB - 141GB H200 HBM3e\n",
    "    - 33C - 35C Temp (current cold)\n",
    "    - 70W draw, 600W limit\n",
    "\n",
    "4. Drivers\n",
    "    - PyTorch 2.9.0+cu128\n",
    "    - torch CUDA 12.8 matches (torch compiled on Cuda 12.8)\n",
    "\n",
    "#### Report\n",
    "- Isolation: Jobs restricted to GPU 2 & 3 via CUDA_VISIBLE_DEVICE=2,3\n",
    "- Compatibility: Driver supports CUDA13.0; PyTorch built for CUDA12.8; CUDA avail\n",
    "- Interconnect: NVLink present (NV6 topo mesh), P2P enabled bidirectionally\n",
    "- Health: Temps low (33-35C), ECC enabled (Error Correction, GPU fixing corruption)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ef977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a42b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2130863e",
   "metadata": {},
   "source": [
    "# Section 2 - GPU Benchmarking Tests\n",
    "\n",
    "1. General MatMul (GEMM) Benchmark (with warmup, CUDA event timing, median/p95) + TFLOPS (math ops / s)\n",
    "    - TFLOPs low -> possible Hardware / Driver problem\n",
    "    - TFLOPs high -> possible code inefficency (data loading / shape mismatch)\n",
    "2. P2P Copy Benchmark (GPU <-> GPU)\n",
    "    - driver update may disable NVLink programatically -> slower performance\n",
    "    - Benchmark test will show 400GBps to 30GBps (PCIe)\n",
    "3. NCCL All-Reduce Benchmark\n",
    "    - not just copying data, sharing and computation of data (ring pattern)  \n",
    "    - if P2P Copy Fast but NCCL slow, possible communication algorithm issue (Ring vs Tree algo)\n",
    "\n",
    "### Idea\n",
    "- when training fails, need to troubleshoot potential issues\n",
    "    1. Math (GEMM)\n",
    "    2. Wire (P2P)\n",
    "    3. Teamwork (NCCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac7e36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, math, socket\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e58f6b",
   "metadata": {},
   "source": [
    "### Section 2 Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c7c26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _visible_device_map_hint():\n",
    "    # With CUDA_VISIBLE_DEVICES=2,3:\n",
    "    # torch device 0 -> physical GPU 2\n",
    "    # torch device 1 -> physical GPU 3\n",
    "    return os.environ.get(\"CUDA_VISIBLE_DEVICES\", None)\n",
    "\n",
    "# Synchronize for CPU Timers - else CPU returns immediately\n",
    "def cuda_sync_all_visible():\n",
    "    if not torch.cuda.is_available(): # no cuda, no sync\n",
    "        return\n",
    "\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.synchronize(i) # force CPU to wait for all running task on GPU to complete\n",
    "\n",
    "def percentile(sorted_vals: list[float], quantile: float) -> float:\n",
    "    if not sorted_vals:\n",
    "        return float('nan')\n",
    "    \n",
    "    # handle floats with precision e.g. 1.0000002  with >= or <=\n",
    "    if quantile <= 0: # 0th percentile -> smallest value in sorted set\n",
    "        return sorted_vals[0]\n",
    "    if quantile >= 1: # 100th percentile -> largest value\n",
    "        return sorted_vals[-1]\n",
    "\n",
    "    # quantile between 0 and 1\n",
    "    idx = int(round((len(sorted_vals) - 1) * quantile))\n",
    "    return sorted_vals[idx]\n",
    "    \n",
    "def summarize_times_ms(times_ms: List[float]) -> Dict[str, float]:\n",
    "    times_ms = sorted(times_ms)\n",
    "    return {\n",
    "        \"median_ms\": percentile(times_ms, 0.50),\n",
    "        \"p95_ms\": percentile(times_ms, 0.95),\n",
    "        \"p99_ms\": percentile(times_ms, 0.99),\n",
    "        \"min_ms\": times_ms[0] if times_ms else float(\"nan\"),\n",
    "        \"max_ms\": times_ms[-1] if times_ms else float(\"nan\"),\n",
    "        \"n\": float(len(times_ms)),\n",
    "    }\n",
    "\n",
    "def time_cuda_op_ms(\n",
    "    op_fn,\n",
    "    device: int,\n",
    "    warmup: int,\n",
    "    iters: int\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Time an operation using CUDA events on specific device idx (visible idx)\n",
    "    op_fn must enqueue work on current device stream\n",
    "\n",
    "    \"\"\"\n",
    "    torch.cuda.set_device(device)\n",
    "    \n",
    "    # warmup gpu\n",
    "    for _ in range(warmup):\n",
    "        op_fn()\n",
    "    \n",
    "    torch.cuda.synchronize(device)\n",
    "    \n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(iters):\n",
    "        start.record()\n",
    "        op_fn()\n",
    "        end.record()\n",
    "        torch.cuda.synchronize(device)\n",
    "        times.append(start.elapsed_time(end)) # in ms\n",
    "    \n",
    "    return times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda33a6",
   "metadata": {},
   "source": [
    "### Section 2 Report Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac0d7793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def section2_meta() -> Dict[str, Any]:\n",
    "    meta = {\n",
    "        \"ts_unix\": time.time(),\n",
    "        \"cuda_visible_devices\": _visible_device_map_hint(),\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"torch_cuda_build\": torch.version.cuda,\n",
    "        \"nccl_version\": torch.cuda.nccl.version() if torch.cuda.is_available() else None,\n",
    "        \"device_count_visible\": torch.cuda.device_count(),\n",
    "        \"gpus\": [],\n",
    "    }\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            p = torch.cuda.get_device_properties(i)\n",
    "            meta[\"gpus\"].append({\n",
    "                \"index\": i,\n",
    "                \"name\": p.name,\n",
    "                \"total_mem_gb\": round(p.total_memory / 1e9, 2),\n",
    "                \"sm_count\": p.multi_processor_count,\n",
    "                \"cc\": f\"{p.major}.{p.minor}\",\n",
    "            })\n",
    "    return meta\n",
    "\n",
    "\n",
    "def mk_row(test_name: str, params: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"section\": \"2\",\n",
    "        \"test_name\": test_name,\n",
    "        \"params\": params,\n",
    "        \"metrics\": metrics,\n",
    "        \"meta\": section2_meta(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c2941",
   "metadata": {},
   "source": [
    "## 2.1 GEMM Benchmark (TFLOPs)\n",
    "- Tera-Floating Point Operations per second\n",
    "- 1 trillion floating point calc per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c082393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GemmCase:\n",
    "    M: int\n",
    "    N: int\n",
    "    K: int\n",
    "    name: str\n",
    "\n",
    "\n",
    "def gemm_tflops(M: int, N: int, K: int, ms: float) -> float:\n",
    "    # 2*M*N*K ops / time\n",
    "    # MatMul of (M x K) * (K x N) gives 2 x M x N x K ops\n",
    "    # 2 because for every element, perform 1 multiplication & 1 addition (fused multiply-add)\n",
    "    sec = ms / 1e3\n",
    "    if sec <= 0:\n",
    "        return float(\"inf\")\n",
    "    return (2.0 * M * N * K) / sec / 1e12\n",
    "\n",
    "\n",
    "def run_gemm_bf16(\n",
    "    cases: List[GemmCase],\n",
    "    device: int = 0,\n",
    "    warmup: int = 10, # allow clock speed to stabilize\n",
    "    iters: int = 30\n",
    ") -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "    for c in cases:\n",
    "        # Allocate once per case\n",
    "        # created outside op() to exclude Memory Management (finding memory to init)\n",
    "        A = torch.randn((c.M, c.K), device=f\"cuda:{device}\", dtype=torch.bfloat16) \n",
    "        B = torch.randn((c.K, c.N), device=f\"cuda:{device}\", dtype=torch.bfloat16)\n",
    "\n",
    "        # op\n",
    "        def op():\n",
    "            # use matmul; BF16 should hit tensor cores\n",
    "            C = A @ B\n",
    "            # prevent lazy elimination\n",
    "            # modern compilers will see C isn't used and delete operation to save time\n",
    "            return C\n",
    "\n",
    "        times_ms = time_cuda_op_ms(op, device=device, warmup=warmup, iters=iters)\n",
    "        stats = summarize_times_ms(times_ms)\n",
    "        tflops_med = gemm_tflops(c.M, c.N, c.K, stats[\"median_ms\"])\n",
    "        tflops_p95 = gemm_tflops(c.M, c.N, c.K, stats[\"p95_ms\"])\n",
    "\n",
    "        rows.append(mk_row(\n",
    "            test_name=\"gemm_bf16\",\n",
    "            params={\"case\": c.name, \"M\": c.M, \"N\": c.N, \"K\": c.K, \"device\": device, \"warmup\": warmup, \"iters\": iters},\n",
    "            metrics={**stats, \"tflops_median\": tflops_med, \"tflops_p95\": tflops_p95}\n",
    "        ))\n",
    "\n",
    "        print(f\"[GEMM BF16] {c.name:16s} M={c.M} N={c.N} K={c.K} | \"\n",
    "              f\"median {stats['median_ms']:.3f} ms | TFLOPS {tflops_med:.2f}\")\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f840ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GEMM Benchmark Scenarios\n",
    "gemm_cases = [\n",
    "    GemmCase(4096, 4096, 4096, \"square_4k\"), # GPU compatible with square matrices\n",
    "    GemmCase(8192, 8192, 8192, \"square_8k\"), # Larger square matrix\n",
    "    GemmCase(8192, 16384, 4096, \"mlp_like\"),     # (tokens) x hidden, hidden x 4hidden (Batch M x Indim x Outdim)\n",
    "    GemmCase(16384, 4096, 4096, \"proj_like\"),    # projection-ish\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5288365e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GEMM BF16] square_4k        M=4096 N=4096 K=4096 | median 0.197 ms | TFLOPS 695.99\n",
      "[GEMM BF16] square_8k        M=8192 N=8192 K=8192 | median 1.534 ms | TFLOPS 716.71\n",
      "[GEMM BF16] mlp_like         M=8192 N=16384 K=4096 | median 1.771 ms | TFLOPS 620.69\n",
      "[GEMM BF16] proj_like        M=16384 N=4096 K=4096 | median 0.895 ms | TFLOPS 614.22\n"
     ]
    }
   ],
   "source": [
    "rows_gemm = run_gemm_bf16(gemm_cases, device=0, warmup=10, iters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e5fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1e6c66b",
   "metadata": {},
   "source": [
    "## 2.2 P2P Copy Benchmark (GB/s)\n",
    "- measure actual data transfer speed between 2 GPUs\n",
    "- determine if system using NVLink or PCIe\n",
    "- 20-60GBps (possibly PCIe)\n",
    "- 150-450GBps (NVLink & bonded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0c1c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_p2p_copy(\n",
    "    sizes_mb: list[int],\n",
    "    warmup: int=20,\n",
    "    iters: int=50,\n",
    "    src_dvc: int=0,\n",
    "    dst_dvc: int=1\n",
    ") -> list[dict[str, Any]]:\n",
    "    assert torch.cuda.device_count() >= 2 # need 2 visible GPUs for P2P\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    # GPU0 -> GPU1 then GPU1 -> GPU0\n",
    "    def one_direction(src: int, dst: int):\n",
    "        torch.cuda.set_device(src)\n",
    "        \n",
    "        # Data payload\n",
    "        # empty generation is instant, focus is on transfer time\n",
    "        for mb in sizes_mb:\n",
    "            n_bytes = mb * 1024 * 1024\n",
    "            n = n_bytes // 2 # bf16 = 2 bytes\n",
    "            src_t = torch.empty((n,), device=f\"cuda:{src}\", dtype=torch.bfloat16)\n",
    "            dst_t = torch.empty((n,), device=f\"cuda:{dst}\", dtype=torch.bfloat16)\n",
    "        \n",
    "        # copy is launched on dst device stream\n",
    "        def op():\n",
    "            with torch.cuda.device(dst):\n",
    "                # copy to destination\n",
    "                dst_t.copy_(src_t, non_blocking=True)\n",
    "        \n",
    "        \n",
    "        # Time on dst device\n",
    "        times_ms = time_cuda_op_ms(op, device=dst, warmup=warmup, iters=iters)\n",
    "        stats = summarize_times_ms(times_ms)\n",
    "        \n",
    "        # Bandwidth in GB/s using median\n",
    "        sec = stats['median_ms'] / 1e3\n",
    "        gbps = (n_bytes / sec) / 1e9 if sec > 0 else float('inf')\n",
    "        rows.append(mk_row(\n",
    "                test_name=\"p2p_copy\",\n",
    "                params={\n",
    "                    \"src\": src, \"dst\": dst,\n",
    "                    \"size_mb\": mb,\n",
    "                    \"warmup\": warmup, \"iters\": iters\n",
    "                },\n",
    "                metrics={**stats, \"gbps_median\": gbps}\n",
    "            ))\n",
    "        \n",
    "        print(f\"[P2P COPY] {src}->{dst} size={mb:4d} MB | median {stats['median_ms']:.3f} ms | {gbps:.2f} GB/s\")\n",
    "\n",
    "    one_direction(src_dvc, dst_dvc)\n",
    "    one_direction(dst_dvc, src_dvc)\n",
    "    return rows\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ea91af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P2P COPY] 0->1 size=4096 MB | median 32.232 ms | 133.25 GB/s\n",
      "[P2P COPY] 1->0 size=4096 MB | median 32.232 ms | 133.25 GB/s\n"
     ]
    }
   ],
   "source": [
    "rows_p2p = run_p2p_copy(\n",
    "    sizes_mb=[4096],\n",
    "    warmup=20,\n",
    "    iters=20,\n",
    "    src_dvc=0,\n",
    "    dst_dvc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f00883fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P2P COPY] 0->1 size=8192 MB | median 64.806 ms | 132.55 GB/s\n",
      "[P2P COPY] 1->0 size=8192 MB | median 64.805 ms | 132.55 GB/s\n"
     ]
    }
   ],
   "source": [
    "rows_p2p = run_p2p_copy(\n",
    "    sizes_mb=[8192],\n",
    "    warmup=20,\n",
    "    iters=30,\n",
    "    src_dvc=0,\n",
    "    dst_dvc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44183b",
   "metadata": {},
   "source": [
    "### P2P benchmark intepretation\n",
    "- Direct GPU-GPU traffic (>90GBps)\n",
    "- Slight asymmetry due to scheduling, clocks, background contention\n",
    "\n",
    "- small data = latency\n",
    "- large data = sustained bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f23d1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97960e63",
   "metadata": {},
   "source": [
    "## 2.3 - NCCL All-Reduce Benchmark\n",
    "- P2P was just copying of data\n",
    "- NCCL involves All-Reduce algorithm (computation + copying)\n",
    "\n",
    "#### torchrun Caveats\n",
    "- running torchrun directly inside notebook cell often hangs / spawns weirdly\n",
    "- run as subprocess from notebook\n",
    "\n",
    "or execute in terminal where nccl_allreduce_benchmark.py resides\n",
    "```\n",
    " CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 nccl_allreduce_benchmark.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "412a0014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0108 12:40:31.737000 3270521 torch/distributed/run.py:803] \n",
      "W0108 12:40:31.737000 3270521 torch/distributed/run.py:803] *****************************************\n",
      "W0108 12:40:31.737000 3270521 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0108 12:40:31.737000 3270521 torch/distributed/run.py:803] *****************************************\n",
      "[Gloo] Rank [Gloo] Rank 1 is connected to 10 peer ranks. Expected number of connected peer ranks is :  is connected to 11\n",
      " peer ranks. Expected number of connected peer ranks is : 1\n",
      "[NCCL AR]    1 MB | med   0.240 ms | p95   0.301 ms | algBW    4.37 GB/s | busBW    4.37 GB/s\n",
      "[NCCL AR]    4 MB | med   0.255 ms | p95   0.288 ms | algBW   16.44 GB/s | busBW   16.44 GB/s\n",
      "[NCCL AR]   16 MB | med   0.381 ms | p95   0.420 ms | algBW   43.99 GB/s | busBW   43.99 GB/s\n",
      "[NCCL AR]   64 MB | med   0.884 ms | p95   0.913 ms | algBW   75.88 GB/s | busBW   75.88 GB/s\n",
      "[NCCL AR]  256 MB | med   2.758 ms | p95   2.832 ms | algBW   97.33 GB/s | busBW   97.33 GB/s\n",
      "[NCCL AR] 1024 MB | med   9.698 ms | p95   9.842 ms | algBW  110.72 GB/s | busBW  110.72 GB/s\n",
      "[W108 12:40:38.544802686 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "BASE_DIR=\"/home/lucasleow/project-titan/proj1_stackvalidation_regressionsuite\"\n",
    "RUNS_DIR=\"${BASE_DIR}/runs\"\n",
    "\n",
    "mkdir -p \"$RUNS_DIR\"\n",
    "\n",
    "# sanity checks (fail fast with clear error)\n",
    "test -d \"$BASE_DIR\"\n",
    "test -f \"$BASE_DIR/nccl_allreduce_benchmark.py\"\n",
    "\n",
    "TS=$(date +%Y%m%d_%H%M%S)\n",
    "\n",
    "export CUDA_VISIBLE_DEVICES=0,1\n",
    "\n",
    "# NCCL \"X-ray\"\n",
    "export NCCL_DEBUG=INFO\n",
    "export NCCL_DEBUG_SUBSYS=INIT,ENV,TOPO\n",
    "export TORCH_DISTRIBUTED_DEBUG=DETAIL\n",
    "\n",
    "export NCCL_DEBUG_FILE=\"${RUNS_DIR}/nccl_${TS}_rank%r.log\"\n",
    "export NCCL_TOPO_DUMP_FILE=\"${RUNS_DIR}/nccl_topo_${TS}.xml\"\n",
    "\n",
    "cd \"$BASE_DIR\"\n",
    "torchrun --standalone --nproc_per_node=2 nccl_allreduce_benchmark.py 2>&1 | tee \"${RUNS_DIR}/nccl_stdout_${TS}.log\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbff737",
   "metadata": {},
   "source": [
    "### NCCL All-Reduce Benchmark\n",
    "- 64MB -> 100.11 GBps (all-reduce moving at that bandwidth) (matches P2P speed)\n",
    "- 256MB -> 54GBps (50% drop) - NCCL switches protocol based on size\n",
    "    - small size -> LL (Low Latency)\n",
    "    - large size -> Ring or Tree \n",
    "    - suspected Bulk Throughput "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785955e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc796926",
   "metadata": {},
   "source": [
    "# Section 3 - Application Sanity Check (vLLM)\n",
    "\n",
    "### Objectives:\n",
    "- GPU can start and idle without problem\n",
    "1. vLLM relies on complex CUDA kernels (PagedAttention)\n",
    "    - PyTorch work != vLLM work because it might be compiled for different CUDA version\n",
    "\n",
    "2. Memory Check\n",
    "    - Loading weights into VRAM + allocating KV Cache requires more memory than raw tensors\n",
    "\n",
    "3. Tokenizer Check\n",
    "    - If CPU-based tokenizer is incompatible with `tokenizer.json`, model will hang or output garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24a21a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from vllm import LLM, SamplingParams\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a7c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "TP_SIZE = 2\n",
    "PROMPT_LEN = 100 # Approximate token count for input\n",
    "OUTPUT_LEN = 100 # Tokens to generate\n",
    "CONCURRENCIES = [1, 16, 32] # Stress Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark():\n",
    "    \n",
    "    # 1. Init Engine\n",
    "    print(f\"Loading {MODEL_ID} with TP={TP_SIZE}\")\n",
    "    llm = LLM(\n",
    "        model=MODEL_ID,\n",
    "        tensor_parallel_size=TP_SIZE, # split model across <TP_SIZE> worker processes\n",
    "        trust_remote_code=True,\n",
    "        gpu_memory_utilization=0.9 # (0.9 * Total) - Weight memory = Allocation for KV Cache\n",
    "    )\n",
    "    \n",
    "    # 2. Define Workload\n",
    "    dummy_prompt = \"Explain the detailed history of the Roman Empire and its fall, including economic factors, military strategies, and political corruption. \" * 3\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=OUTPUT_LEN, # hard limit for num tokens allowed to be generated\n",
    "        temperature=0. # deterministic measurement 0 - always pick highest probability\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 3. Batch Sweep loop\n",
    "    for batch_size in CONCURRENCIES:\n",
    "        print(f\"\\nRunning Batch Size: {batch_size}\")\n",
    "        prompts = [dummy_prompt] * batch_size\n",
    "        \n",
    "        # Synchronize GPU before start\n",
    "        import torch\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "        start_time = time.time() # begin generation\n",
    "        outputs = llm.generate(prompts, sampling_params) # generation\n",
    "        total_time_taken_for_generation = time.time() - start_time # end generation\n",
    "        \n",
    "        # 4. Metric Calc\n",
    "        # Offline mode - Processing work - measure throughput\n",
    "        total_tokens_generated = sum([len(o.outputs[0].token_ids) for o in outputs])\n",
    "        tokens_per_sec = total_tokens_generated / total_time_taken_for_generation\n",
    "        \n",
    "        print(f\"Done in {total_time_taken_for_generation:.2f}s\")\n",
    "        print(f\"Throughput: {tokens_per_sec:.2f} tokens / sec\")\n",
    "        \n",
    "        results[batch_size] = tokens_per_sec\n",
    "        \n",
    "    # 5. Report\n",
    "    print(\"\\n--- Baseline Report Card ---\")\n",
    "    print(\"| Concurrency | Throughput (tok/s) |\")\n",
    "    print(\"|-------------|--------------------|\")\n",
    "    for bs, tps in results.items():\n",
    "        print(f\"| {bs:<11} | {tps:<18.2f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce83b072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-1.5B-Instruct with TP=2\n",
      "INFO 01-08 12:41:44 [utils.py:253] non-default args: {'trust_remote_code': True, 'tensor_parallel_size': 2, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-1.5B-Instruct'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-08 12:41:46 [model.py:514] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 01-08 12:41:46 [model.py:1661] Using max model len 32768\n",
      "INFO 01-08 12:41:48 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m INFO 01-08 12:41:50 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m WARNING 01-08 12:41:50 [multiproc_executor.py:882] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m INFO 01-08 12:41:54 [parallel_state.py:1203] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:45217 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m INFO 01-08 12:41:54 [parallel_state.py:1203] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:45217 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m INFO 01-08 12:41:54 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "[Gloo] Rank [Gloo] Rank 10 is connected to  is connected to 1 peer ranks. 1Expected number of connected peer ranks is : 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m WARNING 01-08 12:41:56 [symm_mem.py:107] SymmMemCommunicator: symmetric memory multicast operations are not supported.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m WARNING 01-08 12:41:56 [symm_mem.py:107] SymmMemCommunicator: symmetric memory multicast operations are not supported.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m INFO 01-08 12:41:56 [parallel_state.py:1411] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m INFO 01-08 12:41:56 [parallel_state.py:1411] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:41:57 [gpu_model_runner.py:3562] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m /home/lucasleow/project-titan/project-titan-venv/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:41:58 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP1 pid=3273345)\u001b[0;0m /home/lucasleow/project-titan/project-titan-venv/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP1 pid=3273345)\u001b[0;0m We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP1 pid=3273345)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b576cfb245b4edc82ee51831c272342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:45:48 [weight_utils.py:487] Time spent downloading weights for Qwen/Qwen2.5-1.5B-Instruct: 229.183872 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:45:49 [weight_utils.py:527] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8895e5b95a4793b1d039630f15fc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:45:49 [default_loader.py:308] Loading weights took 0.19 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP1 pid=3273345)\u001b[0;0m INFO 01-08 12:45:49 [weight_utils.py:527] No model.safetensors.index.json found in remote.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:45:50 [gpu_model_runner.py:3659] Model loading took 1.4490 GiB memory and 231.530852 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:45:54 [backends.py:643] Using cache directory: /home/lucasleow/.cache/vllm/torch_compile_cache/231df38584/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:45:54 [backends.py:703] Dynamo bytecode transform time: 3.13 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:45:58 [backends.py:261] Cache the graph of compile range (1, 16384) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP1 pid=3273345)\u001b[0;0m INFO 01-08 12:45:58 [backends.py:261] Cache the graph of compile range (1, 16384) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:46:01 [backends.py:278] Compiling a graph for compile range (1, 16384) takes 5.78 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:46:01 [monitor.py:34] torch.compile takes 8.91 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:46:03 [gpu_worker.py:375] Available KV cache memory: 118.50 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m INFO 01-08 12:46:03 [kv_cache_utils.py:1291] GPU KV cache size: 8,875,328 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m INFO 01-08 12:46:03 [kv_cache_utils.py:1296] Maximum concurrency for 32,768 tokens per request: 270.85x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:01<00:00, 29.62it/s]\n",
      "Capturing CUDA graphs (decode, FULL):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [00:00<00:00, 52.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP1 pid=3273345)\u001b[0;0m INFO 01-08 12:46:06 [custom_all_reduce.py:216] Registering 5814 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:00<00:00, 57.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:46:06 [custom_all_reduce.py:216] Registering 5814 cuda graph addresses\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=3273343)\u001b[0;0m INFO 01-08 12:46:07 [gpu_model_runner.py:4587] Graph capturing finished in 4 secs, took 0.41 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3273329)\u001b[0;0m INFO 01-08 12:46:07 [core.py:259] init engine (profile, create kv cache, warmup model) took 16.49 seconds\n",
      "INFO 01-08 12:46:09 [llm.py:360] Supported tasks: ['generate']\n",
      "\n",
      "Running Batch Size: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92a38ad060045b7889dde6acefb558f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2c8816ff584a78acb222bf03016345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.28s\n",
      "Throughput: 355.70 tokens / sec\n",
      "\n",
      "Running Batch Size: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29a8c3ebacc44668e2d3f4c91370fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6590c94e904a4d7793a80e65f5cf8c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.28s\n",
      "Throughput: 5715.68 tokens / sec\n",
      "\n",
      "Running Batch Size: 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e2613293a24f74874ccdd5922c05b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd8f15bd1a94bd2889d1b0619e0350c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.30s\n",
      "Throughput: 10539.59 tokens / sec\n",
      "\n",
      "--- ðŸ“Š Baseline Report Card ---\n",
      "| Concurrency | Throughput (tok/s) |\n",
      "|-------------|--------------------|\n",
      "| 1           | 355.70             |\n",
      "| 16          | 5715.68            |\n",
      "| 32          | 10539.59           |\n"
     ]
    }
   ],
   "source": [
    "run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9790d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e60e5ec8",
   "metadata": {},
   "source": [
    "# Section 7 - KV Cache Stress Test\n",
    "- How much conversation history before GPU Crashes?\n",
    "- KV Cache grows linearly with context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-titan-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
